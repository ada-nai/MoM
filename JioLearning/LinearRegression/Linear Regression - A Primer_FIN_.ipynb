{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# plt.style.use(['dark_background'])\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Refresher\n",
    "\n",
    "- What is Machine Learning  \n",
    "\n",
    "    ``` \"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed\" ```  \n",
    "    ~ Arthur Samuel  \n",
    "    \n",
    "- Types of Machine Learning \n",
    "    - Supervised Learning\n",
    "    - Unsupervised Learning\n",
    "    - Semi-supervised Learning\n",
    "    - Reinforcement Learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "- Type of learning where the training set for the model has targets/labels i.e. for each sample in the set, for a given set of features, there is the desired output value provided for the machine to learn\n",
    "- Major types of Supervised Learning:\n",
    "    - Regression\n",
    "    - Classification\n",
    "- Some significant Supervised Learning Algorithms:\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "- Unsupervised Learning is a system of learning wherein the training data is unlabeled\n",
    "- Some significant Unsupervised Learning Algorithms:\n",
    "    - K-means clustering \n",
    "    - Hierarchial Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- The simplest approach to Supervised learning  \n",
    "- The simplest way to relate two variables would be a linear relationship  \n",
    "\n",
    "## Simple Linear Regression\n",
    "- Predicting the output `Y` from a single predictor variable `X`\n",
    "- Mathematically, this is expressed by: $$ y \\approx \\beta_0 + \\beta_1 X  $$\n",
    "- $\\beta_0$: Intercept and  $\\beta_1$: Slope / Coefficient \n",
    "- Analogous to y = b + mx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a random dataset\n",
    "np.random.seed(42)\n",
    "X0 = np.random.rand(100, 1)\n",
    "y0 = 7 + 3 * X0\n",
    "\n",
    "## Plot the dataset\n",
    "plt.plot(X0, y0, 'bo', color= 'r')\n",
    "plt.xlabel('X_0', fontsize= 13)\n",
    "plt.ylabel('y_0', fontsize= 13)\n",
    "plt.title('Points in the 2D space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to **find the best fit for the given line equation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to find the 'Best Fit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Prediction Model (Multiple Linear Regression)\n",
    "\n",
    "$$ h(\\theta) \\approx  \\sum_j \\theta_j x_{j} $$\n",
    "\n",
    "$ h(\\theta) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $  \n",
    "$ h(\\theta) = \\theta^T . X  $  \n",
    "<br>\n",
    "Here:  \n",
    "$ h(\\theta) $: Predicted value for given input vector [ $x_1$ to $x_n$ ]\n",
    "<br>\n",
    "$ \\theta $: Parameter vector [$ \\theta_0 \\,\\,  \\theta_1 \\,\\, \\theta_2 \\,\\, ... \\,\\, \\theta_n $] \n",
    "<br>\n",
    "$ x_0 : 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function - MSE\n",
    "MSE cost function for Linear Regression Model\n",
    "$$ J =  MSE(h_{\\theta}) = \\frac{1}{2m} \\sum_{i=1}^{m}(h({\\theta})^{(i)} - y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "- The optimal parameter vector $ \\theta $ that minimizes the cost function $ MSE(X, h_{\\theta}) $ can be evaluated using a mathematical equation directly. This is called the Normal Equation  \n",
    "$$ \\hat{\\theta} = (X^TX) \\cdot X^T \\cdot y $$\n",
    "\n",
    "Here:  \n",
    "$\\hat{\\theta}$: Optimal value of the parameter vector  \n",
    "$ X $: Input vector  \n",
    "y: Vector of the target values [ $y^{(1)}$ to $y^{(n)}$ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Normal Equation\n",
    "## Generate a random dataset\n",
    "np.random.seed(42)\n",
    "X = 3 * np.random.rand(75, 1)\n",
    "y = 2 + 5 * X + np.random.randn(75, 1) # ùëå ‚âà ùõΩ0 + ùõΩ1‚ãÖùëã + ùúñ -> y = 2 + 5x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the dataset\n",
    "plt.plot(X, y, 'bo', color= 'r')\n",
    "plt.xlabel('X', fontsize= 13)\n",
    "plt.ylabel('y', fontsize= 13)\n",
    "plt.title('Randomly generated dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute ùúÉ_hat\n",
    "X_b = np.c_[np.ones((75, 1)), X]\n",
    "# print(X_b.shape)\n",
    "theta_hat = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) \n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the `?` utility in Jupyter Notebook\n",
    "# np.ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the optimal parameter value \n",
    "theta_hat # Should conform with ùõΩ0= 2, ùõΩ1= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd_theta = np.linalg.pinv(X_b).dot(y) # (ùëãùëá‚ãÖùëã)‚ãÖùëãùëá is the pseudoinverse of X, computed using SVD\n",
    "# svd_thetata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_b.dot(theta_hat)\n",
    "\n",
    "## Plot the fit line\n",
    "plt.plot(X, y, 'bo', color= 'r')\n",
    "plt.plot(X, y_pred, color= 'b')\n",
    "# plt.axis([0, 3, 0, 15])\n",
    "plt.legend(('train points', 'predictions'))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y / y_pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An optimization method for unconstrained optimization problems. \n",
    "- Performs better when there are a large number of instances\n",
    "- Steps to perform gradient descent:\n",
    "    0. Choose Loss function(J), learning rate ($\\alpha$)\n",
    "    1. Initialize parameters with random values (Random initialization)  \n",
    "    2. **Repeat steps 3 through 5 until convergence**:\n",
    "    3. Calculate the gradient of the cost function w.r.t. the parameter $\\nabla J$\n",
    "    4. Substitute the parameter values in the gradient\n",
    "    5. Calculate the new gradient $\\theta_{i_{new}} = \\theta_{i_{old}} - \\alpha \\nabla J$  \n",
    "    NB: $\\alpha \\nabla J$ is also known as the step for descent  \n",
    "    \n",
    "    $ h(\\theta) = \\theta_0 + \\theta_1 x_1 $  \n",
    "    $ J = \\frac{1}{2m} \\sum_{i=1}^{m}(h({\\theta})^{(i)} - y^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GD](bell.png)\n",
    "<a href=\"http://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png\"><p style=\"text-align:center\">Source</p></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Scikit-Learn](scikit-learn-logo-notext.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Scikit-Learn](https://scikit-learn.org/stable/index.html#), better known as `sklearn`, is one of the most prominent ML libraries used in Python \n",
    "<br><br>\n",
    "- Simple and efficient tools for predictive data analysis \n",
    "<br><br>\n",
    "- Accessible to everybody, and reusable in various contexts \n",
    "<br><br>\n",
    "- Built on NumPy, SciPy, and matplotlib \n",
    "<br><br>\n",
    "- Open source, commercially usable - BSD license \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "<br><br>\n",
    "**Via pip**: `pip install --upgrade scikit-learn` \n",
    "<br><br>\n",
    "**Via conda**: `conda install scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations -  Closed Form and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Closed Form\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression() \n",
    "mse = lin_reg.fit(X, y)\n",
    "mse.intercept_, mse.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SGD\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter= 100)\n",
    "sgd_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Fledged example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn datasets\n",
    "The [sklearn.datasets](https://scikit-learn.org/stable/datasets/index.html#:~:text=The%20sklearn.datasets%20package%20embeds%20some%20small%20toy%20datasets,on%20data%20that%20comes%20from%20the%20%E2%80%98real%20world%E2%80%99.) package embeds some small toy datasets \n",
    "<br><br>\n",
    "##### Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boston Housing Dataset](boston.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "housing_ds = datasets.load_boston()\n",
    "housing_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = housing_ds['feature_names']\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing_df = pd.DataFrame(housing_ds['data'], columns= features)\n",
    "print(f'Housing dataset shape: {housing_df.shape}')\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = housing_ds['target']\n",
    "print(type(target))\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "#include random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_df, target, test_size = .15, random_state= 42) #default is 0.25\n",
    "print(f'Train data shape: {X_train.shape}')\n",
    "print(f'Test data shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression() # Ordinary least squares Linear Regression.\n",
    "lr.fit(X_train, y_train)\n",
    "print(f'Intercept: {lr.intercept_} \\n Coefficient vector: {lr.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = lr.predict(X_test)\n",
    "comp = list(zip(y_test, lr_y_pred))\n",
    "comp_df = pd.DataFrame(data= comp, columns= ['y_true', 'y_predicted'])\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lr_mse = mean_squared_error(y_test, lr_y_pred)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2_score\n",
    "- Also known as the coefficient of determination\n",
    "- $ R2\\_score = 1 - \\frac{u}{v}$, where:\n",
    "    - $u = \\sum_{i=1}^m (y - \\hat y)^2$ (residual sum of squares)\n",
    "    - $v =  \\sum_{i=1}^m (y - \\bar y)^2$ (total sum of squares)\n",
    "- General range is [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_r2_score = r2_score(y_test, lr_y_pred)\n",
    "lr_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    \n",
    "    # Plot RMSE\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   \n",
    "    \n",
    "    plt.xlabel(\"Training set size\", fontsize=14) \n",
    "    plt.ylabel(\"RMSE\", fontsize=14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(lr, X_train, y_train)\n",
    "plt.axis([0, 80, 0, 80])                         \n",
    "plt.show()                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(max_iter= 1000, eta0= 0.3) # default hyperparameters\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bleep-Bloop!!!\n",
    "sgd_y_pred = sgd.predict(X_test)\n",
    "comp = list(zip(y_test, sgd_y_pred))\n",
    "comp_df = pd.DataFrame(data= comp, columns= ['y_true', 'y_predicted'])\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, sgd_y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling is important for algorithms like SGD\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sgds = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "sgds.fit(X_train, y_train.ravel())\n",
    "# print(f'Intercept: {sgds.intercept_} \\n Coefficient vector: {sgds.coef_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard score of a sample `x` is calculated as:  \n",
    "\n",
    "$$ z = \\frac{(x - u)}{s} $$  \n",
    "\n",
    "where `u` is the mean of the training samples ,\n",
    "and `s` is the standard deviation of the training samples.\n",
    "\n",
    "- Scaling is applied to all features independently\n",
    "- The sample has mean 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent implementation\n",
    "sgds_y_pred = sgds.predict(X_test)\n",
    "comp = list(zip(y_test, sgds_y_pred))\n",
    "comp_df = pd.DataFrame(data= comp, columns= ['y_true', 'y_predicted'])\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgds_mse = mean_squared_error(y_test, sgds_y_pred)\n",
    "sgds_rmse = np.sqrt(sgds_mse)\n",
    "sgds_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgds_r2_score = r2_score(y_test, sgds_y_pred)\n",
    "sgds_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(sgds, X_train, y_train)\n",
    "plt.axis([0, 80, 0, 100])                         \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants - Regularized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "- Ridge Regression is a regularized version of Linear Regression. \n",
    "- The cost function is modified; an addition is made to it\n",
    "- A regularization term equal to $ \\frac{1}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2 $ is added to the cost function  \n",
    " $$ J = MSE(\\theta)+\\frac{1}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2 $$\n",
    "- NB: $i \\neq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Example code\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha= 1)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Least Absolute Shrinkage and Selection Operator Regression\n",
    "- Similar to Ridge Regression, a regularization term equal to the L1 norm of the parameter vector is added to the cost function\n",
    "- The additional term is: $\\alpha \\sum_{i=1}^{n} |\\theta_i| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso Example code\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha= 1)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "- Elastic Net is a middle-ground between Ridge Regression and Lasso Regression\n",
    "- Along with the regularization hyperparameter $\\alpha$, there is another hyperparamter that controls the ratio of the mix: `r`\n",
    "- The additional term to be added with OLS Linear Regression would be: $ r \\alpha \\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha \\sum_{i=1}^{n} \\theta_i^2  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elastic Net Example code\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha= 0.1, l1_ratio= 0.5)\n",
    "enet.fit(X_train, y_train)\n",
    "enet.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Other regressors in sklearn](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: Problems / Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Warning!!!](xkcd.png)\n",
    "<a href=\"https://www.explainxkcd.com/wiki/index.php/1725:_Linear_Regression\"><p style=\"text-align:center\">Source</p></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Non-linearity of the response-predictor relationships\n",
    "2. Collinearity\n",
    "3. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
